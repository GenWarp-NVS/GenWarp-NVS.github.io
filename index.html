<!DOCTYPE html>
<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-5PZ3MK8QVV"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-5PZ3MK8QVV');
</script>
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>GenWarp</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="img/qual_1.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="912">
    <meta property="og:image:height" content="512">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://GenWarp-NVS.github.io/">
    <meta property="og:title" content="GenWarp: Single Image to Novel Views with Semantic-Preserving Generative Warping">
    <meta property="og:description" content="">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="GenWarp: Single Image to Novel Views with Semantic-Preserving Generative Warping">
    <!-- <meta name="twitter:description" content=""> -->
    <!-- <meta name="twitter:image" content="img/cats.png"> -->


    <!-- mirror: F0%9F%AA%9E&lt -->
    <link rel="icon" type="image/x-icon" href="img/logo.ico">
    <!-- <link rel="icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text y=%22.9em%22 font-size=%2290%22&gt;%E2%9C%A8&lt;/text&gt;&lt;/svg&gt;"> -->
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="./css/twentytwenty.css">

    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/codemirror.min.js"></script>
    <script src="js/clipboard.min.js"></script>
    <script src="js/video_comparison.js"></script>
    <script src="js/app.js"></script>
    <script src="./js/jquery.twentytwenty.js"></script>




</head>


<body>
    <div class="container" id="header" style="text-align: center; margin: auto;">
        <div class="row" id="title-row" style="max-width: 100%; margin: 0 auto; display: inline-block">
            <h2 class="col-md-12 text-center" id="title">
                <b>GenWarp</b>: Single Image to Novel Views with <br>Semantic-Preserving Generative Warping<br>
                <small>
                    Arxiv 2024
                </small>
            </h2>
        </div>
        <div class="row" id="author-row" style="margin:0 auto;">
            <div class="col-md-12 text-center" style="display: table; margin:0 auto">
                <a style="text-decoration:none" href="https://j0seo.github.io/">
                    Junyoung&nbsp;Seo*<sup>,1,3</sup>
                </a>
                <span style="padding-left: 20px;"></span>
                Kazumi Fukuda<sup>1</sup>
                <span style="padding-left: 20px;"></span>
                <!-- <a style="text-decoration:none" href="https://scholar.google.com/citations?user=XCRO260AAAAJ"> -->
                    Takashi Shibuya<sup>1</sup>
                <!-- </a> -->
                <span style="padding-left: 20px;"></span>
                <!-- <a style="text-decoration:none" href="https://scholar.google.com/citations?user=D3h3NxwAAAAJ&hl=en"> -->
                    Takuya Narihira<sup>1</sup>
                <!-- </a> -->
                <br>
                <span style="padding-left: 20px;"></span>
                <!-- <a style="text-decoration:none" href="https://scholar.google.com/citations?user=oyuTmwoAAAAJ"> -->
                    Naoki Murata<sup>1</sup>
                <!-- </a> -->
                
                <span style="padding-left: 20px;"></span>
                <a style="text-decoration:none" href="https://skhu101.github.io">
                    Shoukang Hu<sup>1</sup>
                </a>
                
                <span style="padding-left: 20px;"></span>
                <a style="text-decoration:none" href="https://chiehhsinjesselai.github.io">
                    Chieh-Hsin Lai<sup>1</sup>
                </a>
                <br>
                <span style="padding-left: 20px;"></span>
                <a style="text-decoration:none" href="https://cvlab.korea.ac.kr">
                    Seungryong&nbsp;Kim<sup>†,3</sup>
                </a>
                <span style="padding-left: 20px;"></span>
                <a style="text-decoration:none" href="https://www.yukimitsufuji.com">
                    Yuki Mitsufuji<sup>†,1,2</sup>
                </a>
                <br>
                <table class="author-table" id="author-table">
                    <tr>
                        <td>
                            <sup>1</sup>Sony AI
                        </td>
                        <td>
                            <sup>2</sup>Sony Group Corporation
                        </td>
                        <td>
                            <sup>3</sup>Korea University
                        </td>
                    </tr>
                </table>
                <small>
                    <br>
                    *Work done during an internship at Sony AI.<br>
                    <sup>†</sup>Co-corresponding authors.
                </small>
            </div>
        </div>
    </div>
    <script>
        document.getElementById('author-row').style.maxWidth = document.getElementById("title-row").clientWidth + 'px';
    </script>
    <div class="container" id="main">
        <div class="row">
                <div class="col-sm-6 col-sm-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2405.17251" target="_blank">
                            <img src="./img/paper_image.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                      <li>
                            <!-- <a href="" target="_blank"> -->
                            <a href="https://huggingface.co/spaces/Sony/genwarp" target="_blank">
                                <image src="img/huggingface_logo-noborder.svg" height="60px">
                                <h4><strong>Demo</strong></h4>
                            </a>
                        </li>
                        <li>
                            <!-- <a href="" target="_blank"> -->
                            <a href="https://github.com/sony/genwarp" target="_blank">
                                <image src="img/github.png" height="60px">
                                <h4><strong>Code & Weights</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div class="text-center">
                    <img src="./img/qual_1.png" width="100%">
                </div>

                <div class="text-justify">
                    Our model generates plausible novel views, conditioned on <b>only a single input view</b>, enabling to handle both in-domain images (top) and out-of-domain images (bottom).
                </div>
            </div>
        </div>

        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2"  style="margin-bottom: 5px;">
                <h4>
                    <b>Application: Single Image to 3DGS</b>
                </h4>
                Our model can be applied to various downstream tasks. For example, given a single image, our model generates 3-4 novel view images, followed by feeding them into fast 3DGS reconstructors such as <a href="https://instantsplat.github.io">InstantSplat</a>. Then we can easily obtain a 3DGS scene <b>in 30 seconds</b>.
                <br>
                <div class="text-center">
                <video id="ide" width="70%" playsinline autoplay loop muted controls style="margin-top: 10px;">
                    <source src="video/slide.mp4" type="video/mp4" />
                </div>
            </video>
                <!-- <video id="ide" width="32%" playsinline autoplay loop muted style="margin-top: 10px;">
                        <source src="video/11.mp4" type="video/mp4" />
                </video>
                <video id="ide" width="32%" playsinline autoplay loop muted>
                    <source src="video/22.mp4" type="video/mp4" />
                </video>
                <video id="ide" width="32%" playsinline autoplay loop muted style="margin-top: 10px;">
                    <source src="video/33.mp4" type="video/mp4" />
            </video> -->
            </div>
        </div>

        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h4>
                    <b>Warping-and-Inpainting vs. Ours</b>
                </h4>
                    <p class="text-justify">
                        <!-- The warping-and-inpainting approach successfully generates novel views from in-the-wild images by utilizing large capabilities of large-scale T2I diffusion models. Despite such an advantage, <b>(1)</b> it struggle to handle noisy depth maps predicted by the MDE, 
                        and <b>(2)</b> important semantic details of the input view sometimes get lost during geometric warping.
                        <br> -->
                        We introduce a novel approach where a diffusion model <b>learns to implicitly conduct geometric warping</b> conditioned on MDE depth-based correspondence, instead of warping the pixels or the features directly. We design the model to interactively compensate for the ill-warped regions during its generation process, thereby <b>preventing artifacts typically caused by explicit warping</b>.
                    </p>
                    <div class="text-center">
                        <!-- <video id="ide" width="80%" playsinline autoplay loop muted>
                            <source src="video/web_fig1_short.mp4" type="video/mp4" />
                        </video> -->
                        <img src="./img/concept.jpg" width="85%">
                    </div>
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h4>
                    <b>GenWarp Knows Where to <i>Warp</i> and Where to <i>Refine</i>  </b>
                </h4>
                    <p class="text-justify">
                        In our augmented self-attention, the original self-attention part is more attentive to regions requiring generative priors, such as occluded or ill-warped areas (top), while the cross-view attention part focuses on regions that can be reliably warped from the input view (bottom). By aggregating both attentions at once, the model naturally <b>determines which regions to generate and which to warp.</b></p>
                    <div class="text-center">
                        <!-- <video id="ide" width="80%" playsinline autoplay loop muted>
                            <source src="video/web_fig1_short.mp4" type="video/mp4" />
                        </video> -->
                        <img src="./img/vis.png" width="85%">
                    </div>
            </div>
        </div>
        <br>


        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2"  style="margin-bottom: 5px;">
                <h4>
                    <b>Qualitative Results on In-The-Wild Images</b>
                </h4>
                <div class="text-center">
                    <img src="./img/qual_ood.png" width="100%">
                </div>
            </div>
        </div>

        <br>



        <div class="row">
            <div class="col-md-8 col-md-offset-2"  style="margin-bottom: 5px;">
                <h4>
                    <b>Overall Framework</b>
                </h4>
                <div class="text-justify">
                    Given an input view and a desired camera viewpoint, we obtain a pair of embeddings: a 2D coordinate embedding for the input view, 
                    and a <i>warped</i> coordinate embedding for the novel view.
                    With these embeddings, a semantic preserver network produces a semantic feature of the input view, 
                    and a diffusion model conditioned on them learns to conduct geometric warping 
                    to generate novel views.
                </div>
                <br>

                <div class="text-center">
                    <img src="./img/arch.png" width="85%">
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2"  style="margin-bottom: 5px;">
                <h4>
                    <b>Abstract</b>
                </h4>
                <div style="width: 90%;">
                    <!-- <img src="img/motivation_1.png" style="float: right; width: 40%; height: auto; margin-left: 20px;"> -->
                    <div class="text-justify">
                        Generating novel views from a single image remains a challenging task due to the complexity of 3D scenes and the limited diversity in the existing multi-view datasets to train a model on. Recent research combining large-scale text-to-image (T2I) models with monocular depth estimation (MDE) has shown promise in handling in-the-wild images. In these methods, an input view is geometrically warped to novel views with estimated depth maps, then the warped image is inpainted by T2I models. However, they struggle with noisy depth maps and loss of semantic details when warping an input view to novel viewpoints. In this paper, we propose a novel approach for single-shot novel view synthesis, a semantic-preserving generative warping framework that enables T2I generative models to learn where to warp and where to generate, through augmenting crossview attention with self-attention. Our approach addresses the limitations of existing methods by conditioning the generative model on source view images and incorporating geometric warping signals. Qualitative and quantitative evaluations demonstrate that our model outperforms existing methods in both in-domain and out-of-domain scenarios.
                    </div>
                </div>
                
                <br>
            </div>
        </div>



            <div class="row">
                <div class="col-md-8 col-md-offset-2" style="margin-bottom: 5px;">
                    <h4>
                       <b>Citation</b>
                    </h4>
                    <div class="form-group col-md-10 col-md-offset-1">
                        <textarea id="bibtex" class="form-control" readonly>
@article{seo2024genwarp,
  title={GenWarp: Single Image to Novel Views with Semantic-Preserving Generative Warping},
  author={Seo, Junyoung and Fukuda, Kazumi and Shibuya, Takashi and Narihira, Takuya and Murata, Naoki and Hu, Shoukang and Lai, Chieh-Hsin and Kim, Seungryong and Mitsufuji, Yuki},
  journal={arXiv preprint arXiv:2405.17251},
  year={2024}
}
                        </textarea>
                    </div>
                </div>
            </div>

            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h4>
                        Acknowledgements
                    </h4>
                    <p class="text-justify">
                        We thank <a href="https://github.com/ONground-Korea">Jisang Han</a> for helping with the 3DGS application in this project page. The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a>.
                    </p>
                </div>
            </div>
        </div>


        </div>
        </div>


        


        


</body></html>
